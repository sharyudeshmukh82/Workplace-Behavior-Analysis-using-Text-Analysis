{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharyudeshmukh82/Workplace-Behavior-Analysis-using-Text-Analysis/blob/main/Copy_of_MultiLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Q9AG0gbHU93c",
        "outputId": "c23d7372-b337-4aaa-ec39-58c765c68dec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tflearn\n",
        "!pip install preprocessor"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tflearn in /usr/local/lib/python3.6/dist-packages (0.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tflearn) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tflearn) (1.11.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from tflearn) (4.1.1)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->tflearn) (0.46)\n",
            "Requirement already satisfied: preprocessor in /usr/local/lib/python3.6/dist-packages (1.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-tpNlTdXVA0l"
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import string\n",
        "import tflearn\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import preprocessor as p\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow.contrib import learn\n",
        "from tflearn.data_utils import to_categorical, pad_sequences\n",
        "from keras.models import model_from_json,Sequential\n",
        "from keras.layers import Embedding,Dropout,LSTM,Bidirectional,Dense\n",
        "from keras.utils import np_utils\n",
        "os.environ['KERAS_BACKEND']='tensorflow'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EBQPoKk3VC1d"
      },
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, testX, testY):\n",
        "    temp = model.predict(testX)\n",
        "    y_pred  = np.argmax(temp, 1)\n",
        "    y_true = np.argmax(testY, 1)\n",
        "    precision = metrics.precision_score(y_true, y_pred, average=None)\n",
        "    recall = metrics.recall_score(y_true, y_pred, average=None)\n",
        "    f1_score = metrics.f1_score(y_true, y_pred, average=None)\n",
        "    print(\"Precision: \" + str(precision) + \"\\n\")\n",
        "    print(\"Recall: \" + str(recall) + \"\\n\")\n",
        "    print(\"f1_score: \" + str(f1_score) + \"\\n\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(\":: Classification Report\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    return precision, recall, f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QurcSSJBVFK9"
      },
      "cell_type": "code",
      "source": [
        "def save_model(data,model_type,model,embed_size):\n",
        "    weight_file_name = \"multi.h5\"\n",
        "    model_file_name =   \"multi.json\"\n",
        "    model_json = model.to_json()\n",
        "    model.save(\"multi.json\")\n",
        "    with open(model_file_name, \"w+\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "    # serialize weights to HDF5\n",
        "    model.save_weights(weight_file_name)\n",
        "    print(\"Saved model to disk\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M7RUU27hVHG1"
      },
      "cell_type": "code",
      "source": [
        "def lstm(inp_dim,vocab_size, embed_size, num_classes, learn_rate):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size, embed_size, input_length=inp_dim, trainable=True))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(LSTM(embed_size))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(num_classes,activation='softmax'))\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hdL3sadUVKed"
      },
      "cell_type": "code",
      "source": [
        "def train(data_dict, embed_size,data,dump_embeddings=False):\n",
        "    global NUM_CLASSES,LEARN_RATE,BATCH_SIZE,EPOCHS\n",
        "    data, trainX, trainY, testX, testY, vocab_processor = data_dict[\"data\"], data_dict[\"trainX\"], data_dict[\"trainY\"], data_dict[\"testX\"], data_dict[\"testY\"], data_dict[\"vocab_processor\"]\n",
        "\n",
        "    vocab_size = len(vocab_processor.vocabulary_)\n",
        "    print(\"Vocabulary Size: {:d}\".format(vocab_size))\n",
        "    vocab = vocab_processor.vocabulary_._mapping\n",
        "    print(vocab)\n",
        "    print(\"TrainX shape\",trainX.shape[1])\n",
        "    print(\"Running Model: \" + model_type )\n",
        "    model = lstm(trainX.shape[1], vocab_size, embed_size,8, LEARN_RATE)\n",
        "    print(model.summary())\n",
        "\n",
        "    model.fit(trainX, trainY, epochs=EPOCHS, shuffle=True, batch_size=BATCH_SIZE,\n",
        "                  verbose=1)\n",
        "    save_model(data,model_type,model,embed_size)\n",
        "\n",
        "    return  evaluate_model(model, testX, testY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TF8xs0ZTVNc1"
      },
      "cell_type": "code",
      "source": [
        "def get_train_test(data, x_text, labels):\n",
        "    global NUM_CLASSES\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split( x_text, labels, random_state=42, test_size=0.30)\n",
        "    print(\"Length\",len(X_train))\n",
        "    post_length = np.array([len(x.split(\" \")) for x in x_text])\n",
        "    print(\"Post Length\",post_length)\n",
        "#     from collections import Counter\n",
        "#     print(Counter(post_length))\n",
        "    #take 95 percentile of size as input dim of text\n",
        "    max_document_length = int(np.percentile(post_length,95))\n",
        "\n",
        "\n",
        "    print(\"Document length : \" + str(max_document_length))\n",
        "\n",
        "    #converting words into integers ie representing text input into vectors\n",
        "    #max_document is input dim\n",
        "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length, MAX_FEATURES)\n",
        "    #finds the unique words in corpus\n",
        "    vocab_processor = vocab_processor.fit(x_text)\n",
        "    #convert them into aactual vectors\n",
        "    trainX = np.array(list(vocab_processor.transform(X_train)))\n",
        "    testX = np.array(list(vocab_processor.transform(X_test)))\n",
        "#     print('before seq')\n",
        "#     print(trainX)\n",
        "#     print(testX)\n",
        "#     print(type(Y_train))\n",
        "    trainY = (Y_train).astype('int32')\n",
        "    testY = np.asarray(Y_test)\n",
        "#     print(\"******************************\")\n",
        "#     print(\"Test data labels:\")\n",
        "#     print(\"Type\",type(trainY))\n",
        "#     print(trainY.shape)\n",
        "#     print(testY)\n",
        "    trainX = pad_sequences(trainX, maxlen=max_document_length, value=0.)\n",
        "    testX = pad_sequences(testX, maxlen=max_document_length, value=0.)\n",
        "#     print('after seq')\n",
        "#     print(trainX)\n",
        "#     print(testX)\n",
        "#     trainY = to_categorical(trainY, nb_classes=8)\n",
        "#     testY = to_categorical(testY, nb_classes=8)\n",
        "#     print(\"******************************\")\n",
        "#     print(\"Test data labels after to_categorical:\")\n",
        "#     print(trainY)\n",
        "#     print(testY)\n",
        "    trainY = np_utils.to_categorical(trainY)\n",
        "    testY = np_utils.to_categorical(testY)\n",
        "    print(\"Shape\",trainY.shape)\n",
        "    data_dict = {\n",
        "        \"data\": data,\n",
        "        \"trainX\" : trainX,\n",
        "        \"trainY\" : trainY,\n",
        "        \"testX\" : testX,\n",
        "        \"testY\" : testY,\n",
        "        \"vocab_processor\" : vocab_processor\n",
        "    }\n",
        "\n",
        "    return data_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O4aCpIDoVQBV"
      },
      "cell_type": "code",
      "source": [
        "def load_data(filename):\n",
        "    global HASH_REMOVE\n",
        "    print(\"Loading data from file: \" + filename)\n",
        "    df = pd.read_csv(filename,sep=',')\n",
        "    x_text= np.array(df['Message'])\n",
        "#     print(x_text)\n",
        "#     print(type(data))\n",
        "    labels = np.array(df['Label'])\n",
        "#     print(labels)\n",
        "#     print(type(labels))\n",
        "    return x_text,labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "__FZtXoNVSkl"
      },
      "cell_type": "code",
      "source": [
        "def get_data(data):\n",
        "    global NUM_CLASSES\n",
        "    file_name = 'Class Labelled dataset.csv.txt'\n",
        "\n",
        "    x_text, labels = load_data(file_name)\n",
        "    print(len(x_text))\n",
        "\n",
        "    from collections import Counter\n",
        "    print(Counter(labels))\n",
        "\n",
        "    #remove punctuations\n",
        "    filter_data = []\n",
        "    for text in x_text:\n",
        "        filter_data.append(\"\".join(l for l in text if l not in string.punctuation))\n",
        "#     print(\"Filtered data\",(filter_data[1]))\n",
        "    return filter_data, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ddtZCgtXVVcW"
      },
      "cell_type": "code",
      "source": [
        "def run_model(data, embed_size):\n",
        "    x_text, labels = get_data(data)\n",
        "    data_dict = get_train_test(data,  x_text, labels)\n",
        "    precision, recall, f1_score = train(data_dict, embed_size,data)\n",
        "    print(\"Precision\",precision)\n",
        "    print(\"Recall\",recall)\n",
        "#     print(\"f1-score\",f1_score)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MlhSl9-bW5gl"
      },
      "cell_type": "code",
      "source": [
        "def predict():\n",
        "    file = \"multi.json\"\n",
        "    weight = \"multi.h5\"\n",
        "    data = \"multi\"\n",
        "    oversampling_rate = 3\n",
        "\n",
        "    #load the model\n",
        "    json_file = open(file, 'r')\n",
        "    loaded_model_json = json_file.read()\n",
        "    json_file.close()\n",
        "    loaded_model = model_from_json(loaded_model_json)\n",
        "\n",
        "    # load weights into new model\n",
        "    loaded_model.load_weights(weight)\n",
        "    print(\"Loaded model from disk\")\n",
        "    x_text, labels = get_data(data)\n",
        "    data_dict = get_train_test(data,  x_text, labels)\n",
        "#     print(\"Printing model summary \")\n",
        "    evaluate_model(loaded_model, data_dict['testX'], data_dict['testY'])\n",
        "#     print(x_text[6])\n",
        "\n",
        "    post_length = np.array([len(x.split(\" \")) for x in x_text])\n",
        "\n",
        "    max_document_length = int(np.percentile(post_length,95))\n",
        "\n",
        "    print(\"Document length : \" + str(max_document_length))\n",
        "\n",
        "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length, MAX_FEATURES)\n",
        "    vocab_processor = vocab_processor.fit(x_text)\n",
        "    inp = 0\n",
        "    while inp != \"\":\n",
        "        inp = input()\n",
        "        text = \"\"\n",
        "        for c in inp:\n",
        "            if c not in string.punctuation:\n",
        "                text+=c\n",
        "#         print(text)\n",
        "        text_list = [text]\n",
        "        text_vector = np.array(list(vocab_processor.transform(text_list)))\n",
        "#         print(text_vector)\n",
        "        ans = loaded_model.predict(text_vector)\n",
        "#         print(ans)\n",
        "        prediction= list(ans[0])\n",
        "        print(prediction)\n",
        "        index=prediction.index(max(prediction))\n",
        "#         print(\"index\",index)\n",
        "        class_dict= {1:\"Activities\",2:\"Personal Information\",3:\"Compliment\",4:\"Relationship\",5:\"Reframing\",6:\"Communicative Desensitization\",7:\"Isolation\",0:\"Approach\"}\n",
        "        print(\"Class\",class_dict[index])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AnDiU3DlW-hW",
        "outputId": "b453234b-91dc-460a-8acd-3a0397161a7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1215
        }
      },
      "cell_type": "code",
      "source": [
        "#main function\n",
        "EPOCHS = 15\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "NUM_CLASSES = None\n",
        "DROPOUT = 0.25\n",
        "LEARN_RATE = 0.01\n",
        "MAX_FEATURES =8\n",
        "\n",
        "\n",
        "\n",
        "data = \"multi\"\n",
        "model_type = \"lstm\"\n",
        "embed_size = 128\n",
        "\n",
        "n = input()\n",
        "if n == \"train\":\n",
        "    run_model(data, embed_size)\n",
        "else:\n",
        "    predict()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predict\n",
            "Loaded model from disk\n",
            "Loading data from file: Class Labelled dataset.csv.txt\n",
            "65759\n",
            "Counter({6: 24256, 3: 13735, 7: 11018, 0: 7024, 2: 6685, 5: 2052, 1: 544, 4: 445})\n",
            "Length 46031\n",
            "Post Length [ 6 10  4 ...  7  5 11]\n",
            "Document length : 24\n",
            "Shape (46031, 8)\n",
            "Precision: [0.80907604 0.78651685 0.83879493 0.83470885 0.80508475 0.82743363\n",
            " 0.88488613 0.90708402]\n",
            "\n",
            "Recall: [0.93614002 0.90909091 0.79909366 0.87418968 0.71969697 0.64930556\n",
            " 0.87522184 0.84035409]\n",
            "\n",
            "f1_score: [0.86798246 0.84337349 0.81846313 0.8539932  0.76       0.72762646\n",
            " 0.88002745 0.87244494]\n",
            "\n",
            "[[1979    0   15   34    1   10   50   25]\n",
            " [   1  140    1    5    0    0    3    4]\n",
            " [  48    4 1587   93    4    3  178   69]\n",
            " [ 128    6   33 3641    2   23  303   29]\n",
            " [   3    1    6    7   95    1    4   15]\n",
            " [  10    0   21   26    0  374  110   35]\n",
            " [ 160   12  128  463   13   33 6411  105]\n",
            " [ 117   15  101   93    3    8  186 2753]]\n",
            ":: Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.94      0.87      2114\n",
            "           1       0.79      0.91      0.84       154\n",
            "           2       0.84      0.80      0.82      1986\n",
            "           3       0.83      0.87      0.85      4165\n",
            "           4       0.81      0.72      0.76       132\n",
            "           5       0.83      0.65      0.73       576\n",
            "           6       0.88      0.88      0.88      7325\n",
            "           7       0.91      0.84      0.87      3276\n",
            "\n",
            "   micro avg       0.86      0.86      0.86     19728\n",
            "   macro avg       0.84      0.83      0.83     19728\n",
            "weighted avg       0.86      0.86      0.86     19728\n",
            "\n",
            "Document length : 24\n",
            "meet me on the beach lane\n",
            "[0.9992531, 6.994853e-06, 4.085039e-05, 0.00023449093, 9.5958885e-06, 0.00011560405, 0.0002101506, 0.00012916885]\n",
            "Class Approach\n",
            "lets meet and fuck\n",
            "[0.64134717, 6.454102e-05, 0.0006659425, 0.0019294803, 0.00013364246, 0.00035082296, 0.3550906, 0.00041785382]\n",
            "Class Approach\n",
            "Ill kill you to death\n",
            "[0.048557766, 0.0010720557, 0.12495881, 0.22355032, 0.0063354974, 0.004240208, 0.5336952, 0.05759005]\n",
            "Class Communicative Desensitization\n",
            "I wanna be alone\n",
            "[0.0038657025, 0.0006939189, 0.016867869, 0.033019006, 0.0021559976, 0.0029836446, 0.151621, 0.7887929]\n",
            "Class Isolation\n",
            "You look damn hot in that picture\n",
            "[0.005440594, 0.0002899932, 0.38172767, 0.59769285, 0.0002126706, 0.0006038371, 0.011381296, 0.00265109]\n",
            "Class Compliment\n",
            "hold on \n",
            "[0.073547386, 0.0009203471, 0.10095295, 0.1764781, 0.007989681, 0.004370152, 0.60541314, 0.030328209]\n",
            "Class Communicative Desensitization\n",
            "sweet dreams cutiepie...\n",
            "[5.8172376e-05, 1.480994e-06, 1.812448e-05, 0.9993556, 2.7437088e-06, 2.9233339e-05, 0.0005209906, 1.3600108e-05]\n",
            "Class Compliment\n",
            "freak me baby\n",
            "[0.0005800957, 4.5285284e-05, 0.00023736041, 0.5829855, 2.4814122e-05, 0.00026435478, 0.41549006, 0.00037254588]\n",
            "Class Compliment\n",
            "suckhim\n",
            "[0.058407735, 0.0012604039, 0.10820505, 0.23441666, 0.0071469317, 0.0049854554, 0.5476846, 0.037893105]\n",
            "Class Communicative Desensitization\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}